data_root: /datasets/cv-corpus-15.0/id
model_root: /models/w2v2_bert
data_label: cv15_id

data_preprocessing:
  data_label: ${data_label}
  do_split: true
  no_split:
    input_manifest_path: ${data_root}/manifest.json
    output_manifest_path: ${data_root}/manifest_updated.json
  split:
    input_manifest_path_train: ${data_root}/train_manifest.json
    output_manifest_path_train: ${data_root}/train.json
    input_manifest_path_dev: ${data_root}/dev_manifest.json
    output_manifest_path_dev: ${data_root}/dev.json
    input_manifest_path_test: ${data_root}/test_manifest.json
    output_manifest_path_test: ${data_root}/test.json

bpe_preprocessing:
  train_manifest_dir: ${data_root}/train.json
  dev_manifest_dir: ${data_root}/dev.json
  sentences_text_dir: ${data_root}/sentences.txt
  tokenizer_dir: ${data_root}/tokenizer.json
  tokenizer_vocab_size: 256
  subwords_text_dir: ${data_root}/subwords.txt

build_lm:
  dataset_name: ${data_label}
  n_grams: 5
  kenlm_path: kenlm/
  text_file_path: ${data_root}/kenlm_subwords.txt
  raw_output_lm_path: ${data_root}/${build_lm.n_grams}_grams_${build_lm.dataset_name}.arpa
  corrected_output_lm_path: ${data_root}/${build_lm.n_grams}_grams_${build_lm.dataset_name}_corrected.arpa

finetuning:
  data:
    root_path: ${data_root}
    train_manifest_dir: ${data_root}/train.json
    dev_manifest_dir: ${data_root}/dev.json
    tokenizer_path: ${data_root}/tokenizer.json
    max_token_size: 448 # maximum token length the model would take in to finetune, more than that, do not include into the finetuning train data
    data_label: ${data_label}
  model:
    use_safetensors: true
    pretrained_model_dir: ${model_root}/w2v2_bert_base
    finetuned_output_dir: ${model_root}/w2v2_bert_finetuned_${data_label}_bpe
    resume_from_checkpoint: false
    optimizer:
      is_variable_lr: true
      lr_multiplier: 1.75
    scheduler:
      type: cosine_schedule_with_warmup # cosine_with_hard_restarts_schedule_with_warmup # linear_schedule_with_warmup
  hyperparams:
    attention_dropout: 0.1
    hidden_dropout: 0.0
    feat_proj_dropout: 0.0
    mask_time_prob: 0.05
    final_dropout: 0.1
    layerdrop: 0.1
    ctc_loss_reduction: mean
    add_adapter: true
    num_adapter_layers: 1
    use_intermediate_ffn_before_adapter: false
    lr: 5e-5 # if optimizer.is_variable_lr is set to true, this value will be the initial lr
    weight_decay: 0.01
    warmup_ratio: 0.15
    per_device_train_batch_size: 4
    per_device_eval_batch_size: 4
    gradient_accumulation_steps: 8
    gradient_checkpointing: true
    fp16: true
    num_train_epochs: 8
    save_epoch_interval: 1
    eval_epoch_interval: 1
    logging_epoch_interva1: 1
    save_total_limit: 1
    save_safetensors: true
    ignore_data_skip: true
    greater_is_better: false
    load_best_model_at_end: true
    metric_for_best_model: eval_loss

evaluate_model:
  data:
    root_path: ${data_root}
    test_manifest_dir: ${data_root}/test.json
    data_label: ${data_label}
  model:
    seed: 7
    activate_dropout: false
    feature_extractor_path: ${model_root}/w2v2_bert_finetuned_${data_label}_bpe
    checkpoint: 1092
    saved_model_path: ${evaluate_model.model.feature_extractor_path}/checkpoint-${evaluate_model.model.checkpoint}
    tokenizer_path : ${data_root}/tokenizer.json # ${evaluate_model.model.saved_model_path}/tokenizer.json 
    use_safetensors: ${finetuning.model.use_safetensors}
    attention_dropout: 0.0
    hidden_dropout: 0.0
    feat_proj_dropout: 0.0
    final_dropout: 0.1
    layerdrop: 0.0
    data_loader_batch_size: 8
  results:
    output_pred_path: ${data_root}/pred_results.json
  kenlm:
    model_path: ${data_root}/5_gram_${data_label}_fixed_subwords.arpa
    alpha: 0.6 
    beta: 1.0
    beam_width: 100
    hotwords: # insert yaml list if there are hotwords to be added
    hotword_weight: 10.0

get_metrics_from_json:
  input_json_path: ${data_root}/pred_results.json
  filter:
    type: language # either "language" or "data", or "params" (flexible json params to filter) or leave empty to take into account of the whole dataset
    query: # takes in a list, either the language codes or the data subset names
      - id